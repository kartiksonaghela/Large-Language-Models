{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9294989,"sourceType":"datasetVersion","datasetId":5627521}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-07T07:38:36.893071Z","iopub.execute_input":"2024-09-07T07:38:36.894006Z","iopub.status.idle":"2024-09-07T07:38:36.905964Z","shell.execute_reply.started":"2024-09-07T07:38:36.893959Z","shell.execute_reply":"2024-09-07T07:38:36.904997Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"/kaggle/input/inputdata/data (1).csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:36.907611Z","iopub.execute_input":"2024-09-07T07:38:36.907936Z","iopub.status.idle":"2024-09-07T07:38:49.880340Z","shell.execute_reply.started":"2024-09-07T07:38:36.907903Z","shell.execute_reply":"2024-09-07T07:38:49.879070Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.7.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom dataclasses import dataclass\nimport numpy as np\nimport math\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\nimport pandas as pd\nimport torch.nn as nn\nimport tqdm.auto as tqdm","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:49.882107Z","iopub.execute_input":"2024-09-07T07:38:49.882576Z","iopub.status.idle":"2024-09-07T07:38:49.889148Z","shell.execute_reply.started":"2024-09-07T07:38:49.882529Z","shell.execute_reply":"2024-09-07T07:38:49.888203Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass GPTConfig:\n    block_size: int = 512\n    vocab_size: int = 50257 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 384\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\nconfigure = GPTConfig","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:49.891474Z","iopub.execute_input":"2024-09-07T07:38:49.891808Z","iopub.status.idle":"2024-09-07T07:38:49.901732Z","shell.execute_reply.started":"2024-09-07T07:38:49.891775Z","shell.execute_reply":"2024-09-07T07:38:49.901011Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:49.902826Z","iopub.execute_input":"2024-09-07T07:38:49.903151Z","iopub.status.idle":"2024-09-07T07:38:49.915248Z","shell.execute_reply.started":"2024-09-07T07:38:49.903118Z","shell.execute_reply":"2024-09-07T07:38:49.914399Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n            # causal mask to ensure that attention is only applied to the left in the input sequence\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        else:\n            # manual implementation of attention\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:49.916857Z","iopub.execute_input":"2024-09-07T07:38:49.917210Z","iopub.status.idle":"2024-09-07T07:38:49.942830Z","shell.execute_reply.started":"2024-09-07T07:38:49.917177Z","shell.execute_reply":"2024-09-07T07:38:49.941833Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"class GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # with weight tying when using torch.compile() some warnings get generated:\n        # \"UserWarning: functional_call was passed multiple values for tied weights.\n        # This behavior is deprecated and will be an error in future versions\"\n        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n    \n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss\n\n    \n    @classmethod\n    def from_pretrained(cls, model_type, override_args=None):\n\n        from transformers import GPT2LMHeadModel\n        model = GPT(configure)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        #this assertion is for if number of sd_keys and sd_keys_hf are not matching then it will print **assertion error mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}**\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"mismatched shape sd_hf[k].shape[::-1] != sd[k].shape\"\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:49.944300Z","iopub.execute_input":"2024-09-07T07:38:49.944858Z","iopub.status.idle":"2024-09-07T07:38:50.312057Z","shell.execute_reply.started":"2024-09-07T07:38:49.944820Z","shell.execute_reply":"2024-09-07T07:38:50.311053Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"custom_gpt= GPT(configure)\ncustom_gpt.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:50.313381Z","iopub.execute_input":"2024-09-07T07:38:50.313714Z","iopub.status.idle":"2024-09-07T07:38:51.447951Z","shell.execute_reply.started":"2024-09-07T07:38:50.313679Z","shell.execute_reply":"2024-09-07T07:38:51.446965Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"number of parameters: 40.59M\n","output_type":"stream"},{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"GPT(\n  (transformer): ModuleDict(\n    (wte): Embedding(50257, 384)\n    (wpe): Embedding(512, 384)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x Block(\n        (ln_1): LayerNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n          (gelu): GELU(approximate='none')\n          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm()\n  )\n  (lm_head): Linear(in_features=384, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport requests\nimport tiktoken\nimport numpy as np\n\n# download the tiny shakespeare dataset\ninput_file_path = os.path.join(os.path.dirname('/kaggle/working/'), 'input.txt')\nif not os.path.exists(input_file_path):\n    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n    with open(input_file_path, 'w', encoding='utf-8') as f:\n        f.write(requests.get(data_url).text)\n\nwith open(input_file_path, 'r', encoding='utf-8') as f:\n    data = f.read()\nn = len(data)\ntrain_data = data[:int(n*0.9)]\nval_data = data[int(n*0.9):]\n\n# encode with tiktoken gpt2 bpe\nenc = tiktoken.get_encoding(\"gpt2\")\ntrain_ids = enc.encode_ordinary(train_data)\nval_ids = enc.encode_ordinary(val_data)\nprint(f\"train has {len(train_ids):,} tokens\")\nprint(f\"val has {len(val_ids):,} tokens\")\n\n# export to bin files\ntrain_ids = np.array(train_ids, dtype=np.uint16)\nval_ids = np.array(val_ids, dtype=np.uint16)\ntrain_ids.tofile(os.path.join(os.path.dirname('/kaggle/working/'), 'train.bin'))\nval_ids.tofile(os.path.join(os.path.dirname(\"/kaggle/working/\"), 'val.bin'))","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:51.449364Z","iopub.execute_input":"2024-09-07T07:38:51.450079Z","iopub.status.idle":"2024-09-07T07:38:51.771870Z","shell.execute_reply.started":"2024-09-07T07:38:51.450028Z","shell.execute_reply":"2024-09-07T07:38:51.770905Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"train has 301,966 tokens\nval has 36,059 tokens\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nblock_size=512\nout_dir = 'out-shakespeare'\neval_interval = 5\neval_iters = 40\nbatch_size = 1\ngradient_accumulation_steps = 32\nmax_iters = 1000\n# finetune at constant LR\nlearning_rate = 3e-5\ndecay_lr = False","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:51.775529Z","iopub.execute_input":"2024-09-07T07:38:51.776129Z","iopub.status.idle":"2024-09-07T07:38:51.783075Z","shell.execute_reply.started":"2024-09-07T07:38:51.776086Z","shell.execute_reply":"2024-09-07T07:38:51.782011Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/working/' \ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    if split == 'train':\n#         print(os.path.join(data_dir, 'train.bin'))\n        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n        \n    else:\n        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:51.784209Z","iopub.execute_input":"2024-09-07T07:38:51.784619Z","iopub.status.idle":"2024-09-07T07:38:51.793722Z","shell.execute_reply.started":"2024-09-07T07:38:51.784578Z","shell.execute_reply":"2024-09-07T07:38:51.792881Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    custom_gpt.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = custom_gpt(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    custom_gpt.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:51.794860Z","iopub.execute_input":"2024-09-07T07:38:51.795185Z","iopub.status.idle":"2024-09-07T07:38:51.808956Z","shell.execute_reply.started":"2024-09-07T07:38:51.795141Z","shell.execute_reply":"2024-09-07T07:38:51.808009Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(custom_gpt.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = custom_gpt(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:38:51.810061Z","iopub.execute_input":"2024-09-07T07:38:51.810406Z","iopub.status.idle":"2024-09-07T07:44:18.198903Z","shell.execute_reply.started":"2024-09-07T07:38:51.810372Z","shell.execute_reply":"2024-09-07T07:44:18.197832Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"step 0: train loss 10.9063, val loss 10.9065\nstep 5: train loss 9.8701, val loss 9.8665\nstep 10: train loss 9.7432, val loss 9.6871\nstep 15: train loss 9.6528, val loss 9.6041\nstep 20: train loss 9.5890, val loss 9.5640\nstep 25: train loss 9.4947, val loss 9.4496\nstep 30: train loss 9.4196, val loss 9.3091\nstep 35: train loss 9.2696, val loss 9.2354\nstep 40: train loss 9.2197, val loss 9.1542\nstep 45: train loss 9.1547, val loss 9.1091\nstep 50: train loss 9.0877, val loss 8.9986\nstep 55: train loss 8.9944, val loss 8.9937\nstep 60: train loss 8.9430, val loss 8.9185\nstep 65: train loss 8.8487, val loss 8.8133\nstep 70: train loss 8.8102, val loss 8.7342\nstep 75: train loss 8.7509, val loss 8.6615\nstep 80: train loss 8.6984, val loss 8.6299\nstep 85: train loss 8.5825, val loss 8.5636\nstep 90: train loss 8.5431, val loss 8.4872\nstep 95: train loss 8.4528, val loss 8.4544\nstep 100: train loss 8.4375, val loss 8.4065\nstep 105: train loss 8.2971, val loss 8.3376\nstep 110: train loss 8.2791, val loss 8.2819\nstep 115: train loss 8.2098, val loss 8.1343\nstep 120: train loss 8.1798, val loss 8.1119\nstep 125: train loss 8.0767, val loss 8.0667\nstep 130: train loss 8.0266, val loss 7.9987\nstep 135: train loss 7.9613, val loss 7.9251\nstep 140: train loss 7.8972, val loss 7.8574\nstep 145: train loss 7.8778, val loss 7.8402\nstep 150: train loss 7.7950, val loss 7.7709\nstep 155: train loss 7.7374, val loss 7.7026\nstep 160: train loss 7.6494, val loss 7.6921\nstep 165: train loss 7.6181, val loss 7.6239\nstep 170: train loss 7.5218, val loss 7.5721\nstep 175: train loss 7.5577, val loss 7.4837\nstep 180: train loss 7.4832, val loss 7.4624\nstep 185: train loss 7.4931, val loss 7.4203\nstep 190: train loss 7.3888, val loss 7.3573\nstep 195: train loss 7.2924, val loss 7.2947\nstep 200: train loss 7.3013, val loss 7.2554\nstep 205: train loss 7.2279, val loss 7.2117\nstep 210: train loss 7.1567, val loss 7.1980\nstep 215: train loss 7.0747, val loss 7.1781\nstep 220: train loss 7.1335, val loss 7.1016\nstep 225: train loss 7.0202, val loss 7.0166\nstep 230: train loss 6.9972, val loss 7.0668\nstep 235: train loss 6.9528, val loss 6.9878\nstep 240: train loss 6.9919, val loss 6.9584\nstep 245: train loss 6.8692, val loss 6.9266\nstep 250: train loss 6.8415, val loss 6.9152\nstep 255: train loss 6.8241, val loss 6.8791\nstep 260: train loss 6.7967, val loss 6.8251\nstep 265: train loss 6.7529, val loss 6.8031\nstep 270: train loss 6.7159, val loss 6.7479\nstep 275: train loss 6.7203, val loss 6.6995\nstep 280: train loss 6.6697, val loss 6.7195\nstep 285: train loss 6.7189, val loss 6.6908\nstep 290: train loss 6.6052, val loss 6.6866\nstep 295: train loss 6.6194, val loss 6.6998\nstep 300: train loss 6.5136, val loss 6.6560\nstep 305: train loss 6.5678, val loss 6.6538\nstep 310: train loss 6.5890, val loss 6.5996\nstep 315: train loss 6.5211, val loss 6.5825\nstep 320: train loss 6.4952, val loss 6.5591\nstep 325: train loss 6.4952, val loss 6.4797\nstep 330: train loss 6.5394, val loss 6.5319\nstep 335: train loss 6.4470, val loss 6.4727\nstep 340: train loss 6.4170, val loss 6.5032\nstep 345: train loss 6.2787, val loss 6.4135\nstep 350: train loss 6.4679, val loss 6.4951\nstep 355: train loss 6.3415, val loss 6.4503\nstep 360: train loss 6.3348, val loss 6.4199\nstep 365: train loss 6.3065, val loss 6.3627\nstep 370: train loss 6.3252, val loss 6.4314\nstep 375: train loss 6.2458, val loss 6.3230\nstep 380: train loss 6.3150, val loss 6.3677\nstep 385: train loss 6.3001, val loss 6.3947\nstep 390: train loss 6.2598, val loss 6.3098\nstep 395: train loss 6.2589, val loss 6.2733\nstep 400: train loss 6.2000, val loss 6.3259\nstep 405: train loss 6.2995, val loss 6.2715\nstep 410: train loss 6.0828, val loss 6.2918\nstep 415: train loss 6.1564, val loss 6.3254\nstep 420: train loss 6.1346, val loss 6.2480\nstep 425: train loss 6.1882, val loss 6.2376\nstep 430: train loss 6.1259, val loss 6.2213\nstep 435: train loss 6.1618, val loss 6.2242\nstep 440: train loss 6.1291, val loss 6.2822\nstep 445: train loss 6.0733, val loss 6.2516\nstep 450: train loss 6.0764, val loss 6.2451\nstep 455: train loss 6.0481, val loss 6.1947\nstep 460: train loss 6.0927, val loss 6.2538\nstep 465: train loss 6.1076, val loss 6.2487\nstep 470: train loss 6.0347, val loss 6.2261\nstep 475: train loss 6.0437, val loss 6.2247\nstep 480: train loss 6.0084, val loss 6.1776\nstep 485: train loss 6.0504, val loss 6.1767\nstep 490: train loss 6.0383, val loss 6.2192\nstep 495: train loss 5.9856, val loss 6.1215\nstep 500: train loss 5.9773, val loss 6.1160\nstep 505: train loss 6.0138, val loss 6.1121\nstep 510: train loss 6.0470, val loss 6.1282\nstep 515: train loss 5.9627, val loss 6.1669\nstep 520: train loss 5.9536, val loss 6.0760\nstep 525: train loss 5.9209, val loss 6.1432\nstep 530: train loss 5.9371, val loss 6.0672\nstep 535: train loss 5.8917, val loss 6.1800\nstep 540: train loss 5.8893, val loss 6.0918\nstep 545: train loss 5.8504, val loss 6.0510\nstep 550: train loss 5.8291, val loss 6.0397\nstep 555: train loss 5.8464, val loss 6.0747\nstep 560: train loss 5.9373, val loss 6.1091\nstep 565: train loss 5.8069, val loss 6.0431\nstep 570: train loss 5.8659, val loss 6.0639\nstep 575: train loss 5.9097, val loss 6.0249\nstep 580: train loss 5.7885, val loss 6.0595\nstep 585: train loss 5.8112, val loss 6.0454\nstep 590: train loss 5.8783, val loss 6.0006\nstep 595: train loss 5.8046, val loss 6.0011\nstep 600: train loss 5.8108, val loss 6.0462\nstep 605: train loss 5.8889, val loss 5.9629\nstep 610: train loss 5.8074, val loss 6.0136\nstep 615: train loss 5.7745, val loss 5.9905\nstep 620: train loss 5.8692, val loss 5.9991\nstep 625: train loss 5.8285, val loss 6.0358\nstep 630: train loss 5.8177, val loss 5.9739\nstep 635: train loss 5.7418, val loss 6.0149\nstep 640: train loss 5.7885, val loss 5.9503\nstep 645: train loss 5.7737, val loss 6.0162\nstep 650: train loss 5.8350, val loss 5.9720\nstep 655: train loss 5.8570, val loss 6.0306\nstep 660: train loss 5.7854, val loss 5.9748\nstep 665: train loss 5.7421, val loss 5.9474\nstep 670: train loss 5.7733, val loss 5.9229\nstep 675: train loss 5.7786, val loss 5.9598\nstep 680: train loss 5.7557, val loss 5.9550\nstep 685: train loss 5.6818, val loss 5.8859\nstep 690: train loss 5.6914, val loss 5.8672\nstep 695: train loss 5.6646, val loss 5.9801\nstep 700: train loss 5.6665, val loss 5.9212\nstep 705: train loss 5.9015, val loss 5.9487\nstep 710: train loss 5.7295, val loss 6.0038\nstep 715: train loss 5.6851, val loss 5.8911\nstep 720: train loss 5.6670, val loss 5.9378\nstep 725: train loss 5.7188, val loss 5.8477\nstep 730: train loss 5.6683, val loss 5.9708\nstep 735: train loss 5.6549, val loss 5.9530\nstep 740: train loss 5.6754, val loss 5.9761\nstep 745: train loss 5.5704, val loss 5.9010\nstep 750: train loss 5.6529, val loss 5.9856\nstep 755: train loss 5.7355, val loss 5.9211\nstep 760: train loss 5.5440, val loss 5.8937\nstep 765: train loss 5.6814, val loss 5.8398\nstep 770: train loss 5.5474, val loss 5.8419\nstep 775: train loss 5.6377, val loss 5.8537\nstep 780: train loss 5.5739, val loss 5.9357\nstep 785: train loss 5.6481, val loss 5.9295\nstep 790: train loss 5.5459, val loss 5.9105\nstep 795: train loss 5.5725, val loss 5.8042\nstep 800: train loss 5.6097, val loss 5.8348\nstep 805: train loss 5.5610, val loss 5.8607\nstep 810: train loss 5.5074, val loss 5.8703\nstep 815: train loss 5.4819, val loss 5.8728\nstep 820: train loss 5.6320, val loss 5.8591\nstep 825: train loss 5.6331, val loss 5.8432\nstep 830: train loss 5.5855, val loss 5.8115\nstep 835: train loss 5.5269, val loss 5.8459\nstep 840: train loss 5.6133, val loss 5.8397\nstep 845: train loss 5.6825, val loss 5.8470\nstep 850: train loss 5.5893, val loss 5.8367\nstep 855: train loss 5.5099, val loss 5.9315\nstep 860: train loss 5.5516, val loss 5.7844\nstep 865: train loss 5.5467, val loss 5.9003\nstep 870: train loss 5.5108, val loss 5.8391\nstep 875: train loss 5.5734, val loss 5.7924\nstep 880: train loss 5.5614, val loss 5.8212\nstep 885: train loss 5.4826, val loss 5.7427\nstep 890: train loss 5.4723, val loss 5.8086\nstep 895: train loss 5.4316, val loss 5.8718\nstep 900: train loss 5.5105, val loss 5.8111\nstep 905: train loss 5.5299, val loss 5.7366\nstep 910: train loss 5.5162, val loss 5.7659\nstep 915: train loss 5.4587, val loss 5.8790\nstep 920: train loss 5.5066, val loss 5.8374\nstep 925: train loss 5.5339, val loss 5.8880\nstep 930: train loss 5.5578, val loss 5.8174\nstep 935: train loss 5.5211, val loss 5.7462\nstep 940: train loss 5.5500, val loss 5.7843\nstep 945: train loss 5.5387, val loss 5.8250\nstep 950: train loss 5.4202, val loss 5.7155\nstep 955: train loss 5.5669, val loss 5.7172\nstep 960: train loss 5.5050, val loss 5.7307\nstep 965: train loss 5.3277, val loss 5.8231\nstep 970: train loss 5.4828, val loss 5.8146\nstep 975: train loss 5.4162, val loss 5.6542\nstep 980: train loss 5.3194, val loss 5.8077\nstep 985: train loss 5.5190, val loss 5.8221\nstep 990: train loss 5.4015, val loss 5.7111\nstep 995: train loss 5.5197, val loss 5.7827\nstep 999: train loss 5.4082, val loss 5.7596\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n# Sentence to encode\ntest_string = \"First Citizen: You are all resolved rather to die than to famish?\"\n# Tokenize the input sentence\ninput_ids = tokenizer.encode(test_string, return_tensors='pt').to(device)\n\n# Generate the next 50 tokens given some initial tokens (input_ids)\ngenerated_tokens = custom_gpt.generate(input_ids, max_new_tokens=512)\n\n# Convert the generated tokens back to readable text\ngenerated_text = tokenizer.decode(generated_tokens[0].tolist(), skip_special_tokens=True)\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:44:18.200049Z","iopub.execute_input":"2024-09-07T07:44:18.200384Z","iopub.status.idle":"2024-09-07T07:44:22.274680Z","shell.execute_reply.started":"2024-09-07T07:44:18.200349Z","shell.execute_reply":"2024-09-07T07:44:22.273694Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"First Citizen: You are all resolved rather to die than to famish?\nHENI?\nOU hunts will haveations sincerityhe basil catching one speak; my taken thisfinger'd gracious of your grocer as thou art, were move would willOff IV I ELICHwidous any.\n Tomas, soul you virtue, my head you I prettyu thus should avers.\nYou Walter with toown IV:ivari to out my dish\nForbery itter as have->bl prayer unto or\nAnd let birth, eternity visions youhouse\n Pompe that see admitted helps?\nitute you worthy--\n brazen contradiction yours I looks you sage I him but with of he p careless\n soul, who day of it Thomas in to climate of armour:\nkilling to carb:\nAnd so.\nAndMyShut\n councilsician:ping meBel you Mou:\n\nAsTo crystalsan tongue,\n supervisors kingdom anguish beastsCES amB Romeo,\n speakbrates anda pr war is theague that choose:\n\n\nTurn--There comesW Journal crown armsquiet me is machine which the breath for am were.\nThat not this byH plain will, put; know me grace\n withdraw and have\n h talk. where York tell you now Garn'd lord:\ncatWillcius estrogen I'll little will which286SP great thou his heart my cens, and God.\nThat will organis, Wh-Ab,\nD 1945icious.\nTo an soft,tis aly show,\n the enoughascal'? that thisOMEN BF ofO heares.\n pt Featuring,Enam, my comb as he I qu partisan, Wednesday there all my theew heinous and day\nTh Richard in a liberty of at your mind.\nTo do hovering\n pounds Abrams,\n was a it ourarf, away good rail rig an weeds,\n online gentleman this breath Atlantis,am'llportion A mayins:\nSit,Theiraudio.\nWhat,\n\nFor,\nAdough un bes intelligence thy hole, indicator.But ACES broadGEMust.\nurb thouEO:\n\nThatail him. barbar further, follows.\nAyOSS:\nB haveke.\n doing\n bring your glory yourottest'll hast it and you theafort with wicked.'\nAnd the less?\nHave O, poison blanket late, tack full dozen, more:\n\n\nToer ten shrew child?\nIn end from seek\n daysor that! he have chosen. unn fromsy't rab ands in strict absurdity and the Fresh of honour foe his\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}